# Papers_I_read

## Neural Machine Translation

#### [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)

#### [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

#### ```NMT``` ```'NLP``` [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

#### [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849)

#### [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

#### [Understanding Back-Translation at Scale](https://arxiv.org/abs/1808.09381)

#### [Facebook FAIR's WMT19 News Translation Task Submission](https://arxiv.org/abs/1907.06616)


#### [Online and Linear-Time Attention by Enforcing Monotonic Alignments](https://arxiv.org/abs/1704.00784)

#### - [STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework](https://arxiv.org/abs/1810.08398)

#### - Monotonic Infinite Loopback Attention [Paper](https://arxiv.org/abs/1906.05218) [Review](reviews/milk.md)
  
#### - Attention Forcing for Sequence to Sequence Model Training [Paper](https://arxiv.org/abs/1909.12289) [Review](reviews/att_forcing.md) [OpenReview](https://openreview.net/forum?id=rJe5_CNtPB) 

#### - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [Paper](https://arxiv.org/abs/1901.02860) [Review](reviews/trans_xla.md)
  
#### - Addressing the Under-Translation Problem from the Entropy Perspective [Paper](http://www.nlpr.ia.ac.cn/cip/ZongPublications/2019/2019-ZhaoYang-AAAI.pdf) [Review](reviews/und_trans.md)

#### - The Curious Case of Neural Text Degeneration [Paper](https://arxiv.org/abs/1904.09751) [Review](reviews/neu_degen.md) [OpenReview](https://openreview.net/forum?id=rygGQyrFvH)

#### - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [Paper](https://arxiv.org/abs/1703.03400) [Review](reviews/maml.md)
