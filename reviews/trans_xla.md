
#### - [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- Brief Description and Key points
  + Introduces 'Segement Level Recurrence' to model longer dependencies across the chunks, solves 'context fragmentation' issue
  + Introduces 'Relative Positional Embeddings' for coherent position encoding with segment level recurrence
- Useful Links
  + [Google AI Blog](https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html)
  + Medium: [Towards Data Science](https://towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924) and [Dissecting T-XL](https://medium.com/@mromerocalvo/dissecting-transformer-xl-90963e274bd7)
